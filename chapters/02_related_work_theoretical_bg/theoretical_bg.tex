\chapter{Theoretical background}
\label{chap:theoretical_background}


\section{Zero-copy}
\label{tb:zero_copy}

\section{Endianness}
\label{tb:endianness}

\section{Indexing algorithms}
\label{tb:indexing_algorithms}

\subsection{Indexing Strategy Evaluation}


Several indexing strategies were evaluated to determine the most appropriate approach for the FlatCityBuf format:

\begin{table}[ht]
    \centering
    \caption{Comparison of Indexing Strategies}
    \begin{tabular}{l|c|c|c|c}
        \hline
        \textbf{Strategy} & \textbf{Exact Match} & \textbf{Range Query} & \textbf{Space Efficiency} & \textbf{HTTP Suitability} \\
        \hline
        Hash Tables & $O(1)$ & Poor & Medium & Poor \\
        Sorted Array & $O(\log n)$ & Good & Excellent & Limited \\
        Binary Search Tree & $O(\log n)$ & Good & Good & Limited \\
        B-tree/B+tree & $O(\log_B n)$ & Excellent & Good & Excellent \\
        \hline
    \end{tabular}
    \label{tab:indexing_strategies}
\end{table}

Initial implementation used a sorted array with binary search for its simplicity and space efficiency. However, performance testing revealed significant I/O latency issues when accessing this structure over HTTP, as each binary search step potentially required a separate HTTP request. This insight led to a re-evaluation of the indexing approach.

\section{Binary Search}
\label{tb:binary_search}

Binary search is a fundamental algorithm for finding elements in a sorted array. The classic implementation follows a simple approach: compare the search key with the middle element of the array, then recursively search the left or right half depending on the comparison result \citep{binary_search}.
\begin{algorithm}
  \KwIn{A sorted array, a target value, left and right bounds}
  \KwOut{The index where the target value should be inserted}
  \BlankLine
  \While{left $<$ right}
  {
    mid $\leftarrow$ (left + right) / 2\;
    \If{array[mid] $\geq$ target}
    {
      right $\leftarrow$ mid\;
    }
    \Else
    {
      left $\leftarrow$ mid + 1\;
    }
  }
  \Return{left}
  \caption{Classic Binary Search}%
\label{alg:binary_search}
\end{algorithm}

The time complexity of binary search is logarithmic—the height of the implicit binary search tree is $\log_2(n)$ for an array of size $n$. While this is theoretically efficient, the actual performance suffers when implemented on modern hardware due to memory access patterns. Each comparison requires the processor to fetch a new element, potentially causing a cache miss. In the worst case, the number of memory read operations will be proportional to the height of the tree, with each read potentially requiring access to a different cache line or disk block \citep{binary_search}.

This inefficiency is particularly problematic when binary search is implemented on external memory or over HTTP, where each access incurs significant latency. The sorted array representation with binary search does not take advantage of CPU cache locality, as consecutive comparisons frequently access distant memory locations.

\subsection{Eytzinger Layout}
\label{tb:eytzinger_layout}

While preserving the same algorithmic idea as binary search, the Eytzinger layout (also known as a complete binary tree layout or level-order layout) rearranges the array elements to match the access pattern of a binary search \citep{binary_search}. Instead of storing elements in sorted order, it places them in the order they would be visited during a level-order traversal of a complete binary tree.

This layout significantly improves memory access patterns. When the array is accessed in the sequence of a binary search operation, adjacent accesses often refer to elements that are in the same or adjacent cache lines. This spatial locality enables effective hardware prefetching, allowing the CPU to anticipate and load required data before it is explicitly accessed, thus reducing latency \citep{binary_search}.

The Eytzinger layout can provide up to 4× performance improvement over a standard binary search implementation due to better utilization of the CPU cache hierarchy, despite having the same algorithmic time complexity. This makes it particularly valuable for applications where search operations need to be performed repeatedly on static datasets.

\section{\texorpdfstring{\ac{s+tree}}{S+tree}}
\label{tb:static_btree}

While the Eytzinger layout improves cache utilization for binary search, the number of memory read operations remains proportional to the height of the tree—$\log_2(n)$ for $n$ elements. This is still suboptimal for large datasets, especially when the access pattern involves disk I/O or remote data access \citep{static_b_trees}.

The \ac{s+tree} approach addresses this limitation by fetching multiple keys at once instead of single elements. This aligns with modern hardware characteristics where:

\begin{itemize}
    \item The latency of fetching a single byte is comparable to fetching an entire cache line (64 bytes)
    \item Disk and network I/O operations have high initial latency but relatively low marginal cost for additional bytes
    \item CPU cache lines typically hold multiple array elements (e.g., 16 integers in a 64-byte cache line)
\end{itemize}

The key insight is that loading a block of $B$ elements at once and performing a local search within that block can reduce the total number of cache misses or disk accesses to $\log_B(n)$ instead of $\log_2(n)$—a significant reduction for large datasets \citep{static_b_trees}.

Unlike traditional B+Trees that use explicit pointers between nodes, the Static B+Tree uses an implicit structure where child positions are calculated mathematically. This is possible because:

\begin{itemize}
    \item The tree is constructed once and never modified (static)
    \item The number of elements is known in advance
    \item The tree can be maximally filled with no empty slots
    \item Child positions follow a predictable pattern based on the block size
\end{itemize}

For a \ac{s+tree} with block size $B$, a node with index $k$ has its children at indices calculated by a simple formula: $\text{child}_i(k) = k \cdot (B+1) + i + 1$ for $i \in [0, B]$ \citep{static_b_trees}. This eliminates the need to store and fetch explicit pointer values, further reducing memory usage and improving cache efficiency.

The S+Tree layout achieves up to 15× performance improvement over standard binary search implementations while requiring only 6-7\% additional memory \citep{static_b_trees}. This makes it particularly valuable for applications that perform frequent searches on large, relatively static datasets, especially when accessed over high-latency connections such as disk, network, or HTTP.
